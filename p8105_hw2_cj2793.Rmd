---
title: "p8105_hw2_cj2793"
author: "Chenyu Jin"
date: "2024-09-26"
output: github_document
---

# Problem 1

## Read and clean the data:

First, we should read the excel file, select the columns of interest and convert the entry variable from character (YES vs NO) to a logical variable:

```{r}
library(tidyverse)
cleaned_transit_df = read_csv(file = "hw2data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv", na = c("NA", ".", "")) |>
  janitor::clean_names() |>
  select("line", "station_name", "station_latitude", "station_longitude",
         c("route1":"route11"), "entry", "vending", "entrance_type", "ada") |>
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))
```

This data set contains information about entrances and exits for each subway station in the NYC transit system. The key variables include the subway line (line), station name (station_name), station location (station_latitude, station_longitude), routes served (route1 ~ route11), whether entry is permitted (entry), vending machine availability (vending), entrance type (entrance_type), and ADA compliance (ada). In the cleaning process, we selected these relevant columns from the original data set and converted the entry variable from a character format (YES/NO) to a logical format (TRUE/FALSE). The resulting data set contains 19 columns and retains all 1868 original rows from the source data. The data set is tidy, as each variable is a column, each observation is a row, and each cell contains a single value.

## Answer the following questions using these data:

1. How many distinct stations are there?

```{r}
# We first create a new variable called 'Station', which is the full name of the Station, like '125th St 8th Avenue'

cleaned_transit_df <- cleaned_transit_df |>
  mutate(Station = paste(station_name, line, sep = " "))

num_distinct_stations <- cleaned_transit_df |>
  distinct(Station) |>
  nrow()

num_distinct_stations
```

There are `r num_distinct_stations` stations.

2. How many stations are ADA compliant?

```{r}
ada_status <- cleaned_transit_df |>
  filter(ada == TRUE) |>
  distinct(Station) |>
  nrow()

ada_status
```

There are `r ada_status` stations with ADA compliant.

3. What proportion of station entrances / exits without vending allow entrance?

```{r}
wihout_vending <- cleaned_transit_df |>
  filter(vending == "NO")

entry_status <- wihout_vending |>
  filter(entry == TRUE) |>
  distinct(Station) |>
  nrow()

vending_status <- wihout_vending |> 
  distinct(Station) |>
  nrow()

proportion <- entry_status / vending_status

proportion
```

Proportion of station entrances / exits without vending allow entrance is `r proportion`.

## Reformat data and answer some questions about A train:

```{r}
cleaned_transit_df <- cleaned_transit_df |>
  mutate(across(starts_with("route"), as.character))

long_data <- cleaned_transit_df |>
  pivot_longer(route1:route11, 
               names_to = "route_number", 
               values_to = "route_name") |>
  filter(!is.na(route_name))

# How many distinct stations serve the A train?
A_train_stations <- long_data |>
  filter(route_name == "A") |>
  distinct(Station) |>
  nrow()

A_train_stations

# Of the stations that serve the A train, how many are ADA compliant?
ada_status_A <- long_data |>
  filter(route_name == "A", ada == TRUE) |>
  distinct(Station) |>
  nrow()

ada_status_A
```

`r A_train_stations` distinct stations serve the A train. Of the stations that serve the A train, `r ada_status_A` are ADA compliant.

# Problem 2

## Read and clean the Mr. Trash Wheel sheet:

```{r}
library(tidyverse)
library(readxl)

# specify the sheet in the Excel file and to omit non-data entries (rows with notes / figures; columns containing notes) using arguments in read_excel
file_path <- "hw2data/202309 Trash Wheel Collection Data.xlsx"
Mr_trashwheel_data <- read_excel(file_path, sheet = "Mr. Trash Wheel", skip = 1) |>
  janitor::clean_names()

Mr_trashwheel_data <- Mr_trashwheel_data[, -c(15, 16)] |>
  # omit rows that do not include dumpster-specific data
  filter(!is.na(dumpster)) |>
  # round the number of sports balls to the nearest integer and converts the result to an    integer variable
  mutate(sports_balls = as.integer(round(sports_balls))) |>
  # add an additional variable to keep track of Trash Wheel
  mutate(Trash_Wheel = "Mr. Trash Wheel") |>
  mutate(year = as.numeric(year))
```

## Read and clean the Professor Trash Wheel sheet:

```{r}
# specify the sheet in the Excel file and to omit non-data entries (rows with notes / figures; columns containing notes) using arguments in read_excel
file_path <- "hw2data/202309 Trash Wheel Collection Data.xlsx"
Prof_trashwheel_data <- read_excel(file_path, sheet = "Professor Trash Wheel", skip = 1) |>
  janitor::clean_names()

Prof_trashwheel_data <- Prof_trashwheel_data |>
  # omit rows that do not include dumpster-specific data
  filter(!is.na(dumpster)) |>
  # add an additional variable to keep track of Trash Wheel
  mutate(Trash_Wheel = "Professor Trash Wheel")
```

## Read and clean the Gwynnda Trash Wheel sheet:

```{r}
# specify the sheet in the Excel file and to omit non-data entries (rows with notes / figures; columns containing notes) using arguments in read_excel
file_path <- "hw2data/202309 Trash Wheel Collection Data.xlsx"
Gwy_trashwheel_data <- read_excel(file_path, sheet = "Gwynnda Trash Wheel", skip = 1) |>
  janitor::clean_names()

Gwy_trashwheel_data <- Gwy_trashwheel_data |>
  # omit rows that do not include dumpster-specific data
  filter(!is.na(dumpster)) |>
  # add an additional variable to keep track of Trash Wheel
  mutate(Trash_Wheel = "Gwynnda Trash Wheel")
```

## Combine all datasets into a single tidy dataset:

Because different data sheets have different number of trash types, we should reformat the data structures:

```{r}
Mr_trashwheel_data <- Mr_trashwheel_data |>
 pivot_longer(
      cols = c(plastic_bottles, polystyrene, cigarette_butts, glass_bottles, plastic_bags,                wrappers, sports_balls),
      names_to = "Trash_Type",
      values_to = "Amount"
    )

Prof_trashwheel_data <- Prof_trashwheel_data |>
 pivot_longer(
      cols = c(plastic_bottles, polystyrene, cigarette_butts, glass_bottles, plastic_bags,                wrappers),
      names_to = "Trash_Type",
      values_to = "Amount"
    )

Gwy_trashwheel_data <- Gwy_trashwheel_data |>
 pivot_longer(
      cols = c(plastic_bottles, polystyrene, cigarette_butts, plastic_bags, wrappers),
      names_to = "Trash_Type",
      values_to = "Amount"
    )
```

After changing the data structure, we can combine the data set:

```{r}
# Check the data structure to make sure that corresponding variables in each table has the same data format:
str(Mr_trashwheel_data)
str(Prof_trashwheel_data)
str(Gwy_trashwheel_data)

# After checking, we can combine the data set
combined_data <- bind_rows(Mr_trashwheel_data, Prof_trashwheel_data, Gwy_trashwheel_data)
```

## Write a paragraph about these data:

```{r}
# Count the number of observations in the data set
num_observations <- nrow(combined_data)

num_observations

# Count the distinct number of observation in the data set
num_Mr <- combined_data |>
  filter(Trash_Wheel == "Mr. Trash Wheel") |>
  distinct(dumpster, .keep_all = TRUE) |>
  nrow()

num_Prof <- combined_data |>
  filter(Trash_Wheel == "Professor Trash Wheel") |>
  distinct(dumpster, .keep_all = TRUE) |>
  nrow()

num_Gwy <- combined_data |>
  filter(Trash_Wheel == "Gwynnda Trash Wheel") |>
  distinct(dumpster, date, .keep_all = TRUE) |>
  nrow()

num_total <- num_Mr + num_Prof + num_Gwy

# Total weight of trash collected by Professor Trash Wheel
Prof_trashwheel_weight <- combined_data |>
  filter(Trash_Wheel == "Professor Trash Wheel") |>
  distinct(dumpster, .keep_all = TRUE) |>
  summarize(Total_Weight = sum(weight_tons, na.rm = TRUE)) |>
  pull(Total_Weight)

Prof_trashwheel_weight

# Total number of cigarette butts collected by Gwynnda in June of 2022
Gwy_cigbutts_number <- combined_data |>
  filter(Trash_Wheel == "Gwynnda Trash Wheel" & 
         Trash_Type == "cigarette_butts" & 
         month == "June" & 
         year == 2022) |>
  distinct(dumpster, .keep_all = TRUE) |>
  summarize(Total_Number = sum(Amount, na.rm = TRUE)) |>
  pull(Total_Number)

Gwy_cigbutts_number
```

The data set contains a total of `r num_observations` observations and `r num_total` distinct observations across three trash wheels: Mr. Trash Wheel, Professor Trash Wheel, and Gwynnda Trash Wheel (I include `r num_observations` obs in my combined data set because I transform the data set to a long data table). Each observation represents a record of the amount of trash collected for various categories, such as plastic bottles, polystyrene, cigarette butts, glass bottles, and sports balls. One key variable in the data set is weight_tons, which records the weight of trash collected in tons. For the available data, Professor Trash Wheel collected a total of `r Prof_trashwheel_weight` tons of trash. Additionally, Gwynnda Trash Wheel collected `r Gwy_cigbutts_number` cigarette butts in June of 2022.

# Problem 3

## Import, clean, tidy, and otherwise wrangle bakers, bakes and results datasets:

First, we should import the three datasets:

```{r}
library(tidyverse)
bakers_df = read_csv(file = "hw2data/bakers.csv") |>
  janitor::clean_names()

bakes_df = read.csv(file = "hw2data/bakes.csv") |>
  janitor::clean_names()

results_df = read.csv(file = "hw2data/results.csv") |>
  janitor::clean_names()
```

The results_df looks not so tidy, let's tidy it:

```{r}
results_df <- results_df |>
  slice(3:n()) |> # We only keep the data from the third row
  setNames(c("series", "episode", "baker", "technical", "result")) # Rename the columns
```

Also, in bakes_df, "Jo" should be Jo, without quotation marks:

```{r}
bakes_df[,"baker"] <- gsub('"', '', bakes_df[,"baker"])
```

Now, all three data sets are tidy. We should then check for completeness and correctness across datasets.

## Check for completeness and correctness across datasets:

We should check the consistency of data types as usual.

```{r}
str(bakers_df)
str(bakes_df)
str(results_df)

bakes_df <- bakes_df |>
  mutate(series = as.numeric(series), episode = as.numeric(episode))

results_df <- results_df |>
  mutate(series = as.numeric(series), episode = as.numeric(episode))

bakers_df <- bakers_df |>
  rename(baker = baker_name) |> 
  mutate(series = as.numeric(series))
```

Now, we have ensured the consistency of data types. Then, let's use 'anti_join' to check the completeness and correctness across datasets.

We also notice that in bakers_df, bakers have full name recorded. However, in bakers_df,
bakers have first name recorded. We should make bakers' names consistent:

```{r}
bakers_df <- bakers_df %>%
  mutate(baker = word(baker, 1))
```

Let's check how many records from the results_df that do not have a matching "Series" and "Baker" in the bakers_df and vice versa.

```{r}
rb_number <- anti_join(results_df, bakers_df, by = c("series", "baker")) |>
  nrow()

rb_number

br_number <- anti_join(bakers_df, results_df, by = c("series", "baker")) |>
  nrow()

br_number
```

Here we can see that `r rb_number` records from the results_df that do not have a matching "Series" and "Baker" in the bakers_df; `r br_number` record from the bakers_df that do not have a matching "Series" and "Baker" in the results_df.

After checking, we could merge bakers_df and results_df:

```{r}
# Perform the first merge of bakers and results_clean
bakers_results <- results_df |>
  left_join(bakers_df, by = c("series", "baker"))
```

Then, we consider merge bakes_df and bakers_results:

We should also check how many records from the bakers_results that do not have a matching "Series", "Episode", and "Baker" in the bakes_df and vice versa.

```{r}
mb_number <- anti_join(bakers_results, bakes_df, by = c("series", "episode", "baker")) |>
  nrow()

mb_number

bm_number <- anti_join(bakes_df, bakers_results, by = c("series", "episode", "baker")) |>
  nrow()

bm_number
```

Here we can see that `r mb_number` records from the bakers_results that do not have a matching "Series", "Episode", and "Baker" in the bakes_df; `r bm_number` records from the bakes_df that do not have a matching "Series", "Episode" and "Baker" in the bakes_results.

## Merge and organize a single, final dataset

Finally, we can merge bakers_results and bakes_df to get the final dataset and sort rows by Series, Episode, and Baker to ensure meaningful order of observations.

```{r}
final_data <- bakers_results |>
  left_join(bakes_df, by = c("series", "episode", "baker")) |>
  select(series, episode, baker, baker_age, baker_occupation, hometown, 
         signature_bake, show_stopper, technical, result) |>
  arrange(series, episode, baker)
```

## Export the final data set

```{r}
write_csv(final_data, "hw2data/final_bakeoff_data.csv")
```

## Describe the data cleaning process and briefly discuss the final dataset

The data cleaning process began by importing three datasets: bakers.csv, bakes.csv, and results.csv. Each dataset provided a different aspect of information about the Great British Bake Off contestants. Once the datasets were imported, the first task was to clean the results.csv file, which had irrelevant information in the first two rows and incorrect column names. To fix this, the first two rows were removed, and the columns were renamed to series, episode, baker, technical, and result. Also, we noticed that in bakes.csv file, "Jo" should be Jo, without quotation marks.

Another issue encountered was the inconsistency in data types for the series and episode columns across the datasets. In some files, these were stored as integer values, while in others, they were character values. To standardize the data, these columns were converted to numeric type across all datasets, ensuring that the merging process could proceed smoothly. One of the more significant challenges arose from the fact that the baker column in bakes.csv and results.csv contained only first names, while in bakers.csv, full names were used. To resolve this, the first names were extracted from the baker column in bakers.csv using the mutate() and word() functions. This allowed the datasets to be merged using the correct identifier.

After the first step of merging the cleaned results.csv file with bakers.csv, the second merge was performed with bakes.csv, using both series, episode, and baker to match contestants' demographic information to their performance data.

Once the datasets were merged, the columns were reorganized for better readability. The baker age, occupation and hometown columns were moved immediately after the baker column. To further enhance the structure of the dataset, the data was sorted by series, episode, and baker, ensuring a logical flow that made it easy to track each contestantâ€™s progress throughout the competition.

Finally, the cleaned and organized dataset was exported as final_bakeoff_data.csv. The resulting dataset includes detailed demographic information about each baker, such as their first name, age, occupation, and hometown, along with performance data for each episode in each series, including signature and showstopper bakes, technical rankings, and overall episode results. The logical structure of the dataset allows for easy tracking of individual contestants and their journeys across seasons.

## Create a reader-friendly table

```{r}
starbakers_winners <- final_data |>
  filter(series %in% c("5", "6", "7", "8", "9", "10") & 
         result %in% c("STAR BAKER", "WINNER")) |>
  select(series, episode, baker, baker_age, baker_occupation, hometown, result) %>%
  arrange(series, episode)

winners <- starbakers_winners |>
  filter(result == "WINNER")
```

Most of the winners appear to be in their 30s, except for Nancy, who won Season 5 at the age of 60. 
To our surprise, the competition is highly dynamic and the overall winner isn't always the clear front runner throughout the season.

## Import, clean, tidy, and organize the viewership data

```{r}
viewers_df = read.csv(file = "hw2data/viewers.csv") |>
  janitor::clean_names()
head(viewers_df, 10)
```

What was the average viewership in Season 1? In Season 5?
```{r}
Series1 = viewers_df |>
  pull(series_1)
S1_mean = mean(Series1, na.rm = TRUE)

Series5 = viewers_df |>
  pull(series_5)
S5_mean = mean(Series5, na.rm = TRUE)
```

The average viewership in Season 1 was `r S1_mean`, in Season 5 was `r S5_mean`.