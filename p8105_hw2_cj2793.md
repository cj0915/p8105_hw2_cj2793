p8105_hw2_cj2793
================
Chenyu Jin
2024-09-26

# Problem 1

## Read and clean the data:

First, we should read the excel file, select the columns of interest and
convert the entry variable from character (YES vs NO) to a logical
variable:

``` r
library(tidyverse)
```

    ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
    ## ✔ dplyr     1.1.4     ✔ readr     2.1.5
    ## ✔ forcats   1.0.0     ✔ stringr   1.5.1
    ## ✔ ggplot2   3.5.1     ✔ tibble    3.2.1
    ## ✔ lubridate 1.9.3     ✔ tidyr     1.3.1
    ## ✔ purrr     1.0.2     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

``` r
cleaned_transit_df = read_csv(file = "hw2data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv", na = c("NA", ".", "")) |>
  janitor::clean_names() |>
  select("line", "station_name", "station_latitude", "station_longitude",
         c("route1":"route11"), "entry", "vending", "entrance_type", "ada") |>
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))
```

    ## Rows: 1868 Columns: 32
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (22): Division, Line, Station Name, Route1, Route2, Route3, Route4, Rout...
    ## dbl  (8): Station Latitude, Station Longitude, Route8, Route9, Route10, Rout...
    ## lgl  (2): ADA, Free Crossover
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

This data set contains information about entrances and exits for each
subway station in the NYC transit system. The key variables include the
subway line (line), station name (station_name), station location
(station_latitude, station_longitude), routes served (route1 ~ route11),
whether entry is permitted (entry), vending machine availability
(vending), entrance type (entrance_type), and ADA compliance (ada). In
the cleaning process, we selected these relevant columns from the
original data set and converted the entry variable from a character
format (YES/NO) to a logical format (TRUE/FALSE). The resulting data set
contains 19 columns and retains all 1868 original rows from the source
data. The data set is tidy, as each variable is a column, each
observation is a row, and each cell contains a single value.

## Answer the following questions using these data:

1.  How many distinct stations are there?

``` r
# We first create a new variable called 'Station', which is the full name of the Station, like '125th St 8th Avenue'

cleaned_transit_df <- cleaned_transit_df |>
  mutate(Station = paste(station_name, line, sep = " "))

num_distinct_stations <- cleaned_transit_df |>
  distinct(Station) |>
  nrow()

num_distinct_stations
```

    ## [1] 465

There are 465 stations.

2.  How many stations are ADA compliant?

``` r
ada_status <- cleaned_transit_df |>
  filter(ada == TRUE) |>
  distinct(Station) |>
  nrow()

ada_status
```

    ## [1] 84

There are 84 stations with ADA compliant.

3.  What proportion of station entrances / exits without vending allow
    entrance?

``` r
wihout_vending <- cleaned_transit_df |>
  filter(vending == "NO")

entry_status <- wihout_vending |>
  filter(entry == TRUE) |>
  distinct(Station) |>
  nrow()

vending_status <- wihout_vending |> 
  distinct(Station) |>
  nrow()

proportion <- entry_status / vending_status

proportion
```

    ## [1] 0.4343434

Proportion of station entrances / exits without vending allow entrance
is 0.4343434.

## Reformat data and answer some questions about A train:

``` r
cleaned_transit_df <- cleaned_transit_df |>
  mutate(across(starts_with("route"), as.character))

long_data <- cleaned_transit_df |>
  pivot_longer(route1:route11, 
               names_to = "route_number", 
               values_to = "route_name") |>
  filter(!is.na(route_name))

# How many distinct stations serve the A train?
A_train_stations <- long_data |>
  filter(route_name == "A") |>
  distinct(Station) |>
  nrow()

A_train_stations
```

    ## [1] 60

``` r
# Of the stations that serve the A train, how many are ADA compliant?
ada_status_A <- long_data |>
  filter(route_name == "A", ada == TRUE) |>
  distinct(Station) |>
  nrow()

ada_status_A
```

    ## [1] 17

60 distinct stations serve the A train. Of the stations that serve the A
train, 17 are ADA compliant.

# Problem 2

## Read and clean the Mr. Trash Wheel sheet:

``` r
library(tidyverse)
library(readxl)

# specify the sheet in the Excel file and to omit non-data entries (rows with notes / figures; columns containing notes) using arguments in read_excel
file_path <- "hw2data/202309 Trash Wheel Collection Data.xlsx"
Mr_trashwheel_data <- read_excel(file_path, sheet = "Mr. Trash Wheel", skip = 1) |>
  janitor::clean_names()
```

    ## New names:
    ## • `` -> `...15`
    ## • `` -> `...16`

``` r
Mr_trashwheel_data <- Mr_trashwheel_data[, -c(15, 16)] |>
  # omit rows that do not include dumpster-specific data
  filter(!is.na(dumpster)) |>
  # round the number of sports balls to the nearest integer and converts the result to an    integer variable
  mutate(sports_balls = as.integer(round(sports_balls))) |>
  # add an additional variable to keep track of Trash Wheel
  mutate(Trash_Wheel = "Mr. Trash Wheel") |>
  mutate(year = as.numeric(year))
```

## Read and clean the Professor Trash Wheel sheet:

``` r
# specify the sheet in the Excel file and to omit non-data entries (rows with notes / figures; columns containing notes) using arguments in read_excel
file_path <- "hw2data/202309 Trash Wheel Collection Data.xlsx"
Prof_trashwheel_data <- read_excel(file_path, sheet = "Professor Trash Wheel", skip = 1) |>
  janitor::clean_names()

Prof_trashwheel_data <- Prof_trashwheel_data |>
  # omit rows that do not include dumpster-specific data
  filter(!is.na(dumpster)) |>
  # add an additional variable to keep track of Trash Wheel
  mutate(Trash_Wheel = "Professor Trash Wheel")
```

## Read and clean the Gwynnda Trash Wheel sheet:

``` r
# specify the sheet in the Excel file and to omit non-data entries (rows with notes / figures; columns containing notes) using arguments in read_excel
file_path <- "hw2data/202309 Trash Wheel Collection Data.xlsx"
Gwy_trashwheel_data <- read_excel(file_path, sheet = "Gwynnda Trash Wheel", skip = 1) |>
  janitor::clean_names()

Gwy_trashwheel_data <- Gwy_trashwheel_data |>
  # omit rows that do not include dumpster-specific data
  filter(!is.na(dumpster)) |>
  # add an additional variable to keep track of Trash Wheel
  mutate(Trash_Wheel = "Gwynnda Trash Wheel")
```

## Combine all datasets into a single tidy dataset:

Because different data sheets have different number of trash types, we
should reformat the data structures:

``` r
Mr_trashwheel_data <- Mr_trashwheel_data |>
 pivot_longer(
      cols = c(plastic_bottles, polystyrene, cigarette_butts, glass_bottles, plastic_bags,                wrappers, sports_balls),
      names_to = "Trash_Type",
      values_to = "Amount"
    )

Prof_trashwheel_data <- Prof_trashwheel_data |>
 pivot_longer(
      cols = c(plastic_bottles, polystyrene, cigarette_butts, glass_bottles, plastic_bags,                wrappers),
      names_to = "Trash_Type",
      values_to = "Amount"
    )

Gwy_trashwheel_data <- Gwy_trashwheel_data |>
 pivot_longer(
      cols = c(plastic_bottles, polystyrene, cigarette_butts, plastic_bags, wrappers),
      names_to = "Trash_Type",
      values_to = "Amount"
    )
```

After changing the data structure, we can combine the data set:

``` r
# Check the data structure to make sure that corresponding variables in each table has the same data format:
str(Mr_trashwheel_data)
```

    ## tibble [4,088 × 10] (S3: tbl_df/tbl/data.frame)
    ##  $ dumpster          : num [1:4088] 1 1 1 1 1 1 1 2 2 2 ...
    ##  $ month             : chr [1:4088] "May" "May" "May" "May" ...
    ##  $ year              : num [1:4088] 2014 2014 2014 2014 2014 ...
    ##  $ date              : POSIXct[1:4088], format: "2014-05-16" "2014-05-16" ...
    ##  $ weight_tons       : num [1:4088] 4.31 4.31 4.31 4.31 4.31 4.31 4.31 2.74 2.74 2.74 ...
    ##  $ volume_cubic_yards: num [1:4088] 18 18 18 18 18 18 18 13 13 13 ...
    ##  $ homes_powered     : num [1:4088] 0 0 0 0 0 0 0 0 0 0 ...
    ##  $ Trash_Wheel       : chr [1:4088] "Mr. Trash Wheel" "Mr. Trash Wheel" "Mr. Trash Wheel" "Mr. Trash Wheel" ...
    ##  $ Trash_Type        : chr [1:4088] "plastic_bottles" "polystyrene" "cigarette_butts" "glass_bottles" ...
    ##  $ Amount            : num [1:4088] 1450 1820 126000 72 584 ...

``` r
str(Prof_trashwheel_data)
```

    ## tibble [636 × 10] (S3: tbl_df/tbl/data.frame)
    ##  $ dumpster          : num [1:636] 1 1 1 1 1 1 2 2 2 2 ...
    ##  $ month             : chr [1:636] "January" "January" "January" "January" ...
    ##  $ year              : num [1:636] 2017 2017 2017 2017 2017 ...
    ##  $ date              : POSIXct[1:636], format: "2017-01-02" "2017-01-02" ...
    ##  $ weight_tons       : num [1:636] 1.79 1.79 1.79 1.79 1.79 1.79 1.58 1.58 1.58 1.58 ...
    ##  $ volume_cubic_yards: num [1:636] 15 15 15 15 15 15 15 15 15 15 ...
    ##  $ homes_powered     : num [1:636] 29.8 29.8 29.8 29.8 29.8 ...
    ##  $ Trash_Wheel       : chr [1:636] "Professor Trash Wheel" "Professor Trash Wheel" "Professor Trash Wheel" "Professor Trash Wheel" ...
    ##  $ Trash_Type        : chr [1:636] "plastic_bottles" "polystyrene" "cigarette_butts" "glass_bottles" ...
    ##  $ Amount            : num [1:636] 1950 6080 19700 8 3100 ...

``` r
str(Gwy_trashwheel_data)
```

    ## tibble [775 × 10] (S3: tbl_df/tbl/data.frame)
    ##  $ dumpster          : num [1:775] 1 1 1 1 1 2 2 2 2 2 ...
    ##  $ month             : chr [1:775] "July" "July" "July" "July" ...
    ##  $ year              : num [1:775] 2021 2021 2021 2021 2021 ...
    ##  $ date              : POSIXct[1:775], format: "2021-07-03" "2021-07-03" ...
    ##  $ weight_tons       : num [1:775] 0.93 0.93 0.93 0.93 0.93 2.26 2.26 2.26 2.26 2.26 ...
    ##  $ volume_cubic_yards: num [1:775] 15 15 15 15 15 15 15 15 15 15 ...
    ##  $ homes_powered     : num [1:775] 15.5 15.5 15.5 15.5 15.5 ...
    ##  $ Trash_Wheel       : chr [1:775] "Gwynnda Trash Wheel" "Gwynnda Trash Wheel" "Gwynnda Trash Wheel" "Gwynnda Trash Wheel" ...
    ##  $ Trash_Type        : chr [1:775] "plastic_bottles" "polystyrene" "cigarette_butts" "plastic_bags" ...
    ##  $ Amount            : num [1:775] 1200 360 3400 1800 NA 2000 240 3900 2200 NA ...

``` r
# After checking, we can combine the data set
combined_data <- bind_rows(Mr_trashwheel_data, Prof_trashwheel_data, Gwy_trashwheel_data)
```

## Write a paragraph about these data:

``` r
# Count the number of observations in the data set
num_observations <- nrow(combined_data)

num_observations
```

    ## [1] 5499

``` r
# Count the distinct number of observation in the data set
num_Mr <- combined_data |>
  filter(Trash_Wheel == "Mr. Trash Wheel") |>
  distinct(dumpster, .keep_all = TRUE) |>
  nrow()

num_Prof <- combined_data |>
  filter(Trash_Wheel == "Professor Trash Wheel") |>
  distinct(dumpster, .keep_all = TRUE) |>
  nrow()

num_Gwy <- combined_data |>
  filter(Trash_Wheel == "Gwynnda Trash Wheel") |>
  distinct(dumpster, date, .keep_all = TRUE) |>
  nrow()

num_total <- num_Mr + num_Prof + num_Gwy

# Total weight of trash collected by Professor Trash Wheel
Prof_trashwheel_weight <- combined_data |>
  filter(Trash_Wheel == "Professor Trash Wheel") |>
  distinct(dumpster, .keep_all = TRUE) |>
  summarize(Total_Weight = sum(weight_tons, na.rm = TRUE)) |>
  pull(Total_Weight)

Prof_trashwheel_weight
```

    ## [1] 216.26

``` r
# Total number of cigarette butts collected by Gwynnda in June of 2022
Gwy_cigbutts_number <- combined_data |>
  filter(Trash_Wheel == "Gwynnda Trash Wheel" & 
         Trash_Type == "cigarette_butts" & 
         month == "June" & 
         year == 2022) |>
  distinct(dumpster, .keep_all = TRUE) |>
  summarize(Total_Number = sum(Amount, na.rm = TRUE)) |>
  pull(Total_Number)

Gwy_cigbutts_number
```

    ## [1] 18120

The data set contains a total of 5499 observations and 845 distinct
observations across three trash wheels: Mr. Trash Wheel, Professor Trash
Wheel, and Gwynnda Trash Wheel (I include 5499 obs in my combined data
set because I transform the data set to a long data table). Each
observation represents a record of the amount of trash collected for
various categories, such as plastic bottles, polystyrene, cigarette
butts, glass bottles, and sports balls. One key variable in the data set
is weight_tons, which records the weight of trash collected in tons. For
the available data, Professor Trash Wheel collected a total of 216.26
tons of trash. Additionally, Gwynnda Trash Wheel collected 1.812^{4}
cigarette butts in June of 2022.

# Problem 3

## Import, clean, tidy, and otherwise wrangle bakers, bakes and results datasets:

First, we should import the three datasets:

``` r
library(tidyverse)
bakers_df = read_csv(file = "hw2data/bakers.csv") |>
  janitor::clean_names()
```

    ## Rows: 120 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker Name, Baker Occupation, Hometown
    ## dbl (2): Series, Baker Age
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
bakes_df = read.csv(file = "hw2data/bakes.csv") |>
  janitor::clean_names()

results_df = read.csv(file = "hw2data/results.csv") |>
  janitor::clean_names()
```

The results_df looks not so tidy, let’s tidy it:

``` r
results_df <- results_df |>
  slice(3:n()) |> # We only keep the data from the third row
  setNames(c("series", "episode", "baker", "technical", "result")) # Rename the columns
```

Also, in bakes_df, “Jo” should be Jo, without quotation marks:

``` r
bakes_df[,"baker"] <- gsub('"', '', bakes_df[,"baker"])
```

Now, all three data sets are tidy. We should then check for completeness
and correctness across datasets.

## Check for completeness and correctness across datasets:

We should check the consistency of data types as usual.

``` r
str(bakers_df)
```

    ## spc_tbl_ [120 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)
    ##  $ baker_name      : chr [1:120] "Ali Imdad" "Alice Fevronia" "Alvin Magallanes" "Amelia LeBruin" ...
    ##  $ series          : num [1:120] 4 10 6 10 7 1 9 4 2 7 ...
    ##  $ baker_age       : num [1:120] 25 28 37 24 25 30 30 31 31 23 ...
    ##  $ baker_occupation: chr [1:120] "Charity worker" "Geography teacher" "Nurse" "Fashion designer" ...
    ##  $ hometown        : chr [1:120] "Saltley, Birmingham" "Essex" "Bracknell, Berkshire" "Halifax" ...
    ##  - attr(*, "spec")=
    ##   .. cols(
    ##   ..   `Baker Name` = col_character(),
    ##   ..   Series = col_double(),
    ##   ..   `Baker Age` = col_double(),
    ##   ..   `Baker Occupation` = col_character(),
    ##   ..   Hometown = col_character()
    ##   .. )
    ##  - attr(*, "problems")=<externalptr>

``` r
str(bakes_df)
```

    ## 'data.frame':    548 obs. of  5 variables:
    ##  $ series        : int  1 1 1 1 1 1 1 1 1 1 ...
    ##  $ episode       : int  1 1 1 1 1 1 1 1 1 1 ...
    ##  $ baker         : chr  "Annetha" "David" "Edd" "Jasminder" ...
    ##  $ signature_bake: chr  "Light Jamaican Black Cakewith Strawberries and Cream" "Chocolate Orange Cake" "Caramel Cinnamon and Banana Cake" "Fresh Mango and Passion Fruit Hummingbird Cake" ...
    ##  $ show_stopper  : chr  "Red, White & Blue Chocolate Cake with Cigarellos, Fresh Fruit, and Cream" "Black Forest Floor Gateauxwith Moulded Chocolate Leaves, Fallen Fruitand Chocolate Mushrooms Moulded from eggs" "N/A" "N/A" ...

``` r
str(results_df)
```

    ## 'data.frame':    1136 obs. of  5 variables:
    ##  $ series   : chr  "1" "1" "1" "1" ...
    ##  $ episode  : chr  "1" "1" "1" "1" ...
    ##  $ baker    : chr  "Annetha" "David" "Edd" "Jasminder" ...
    ##  $ technical: chr  "2" "3" "1" NA ...
    ##  $ result   : chr  "IN" "IN" "IN" "IN" ...

``` r
bakes_df <- bakes_df |>
  mutate(series = as.numeric(series), episode = as.numeric(episode))

results_df <- results_df |>
  mutate(series = as.numeric(series), episode = as.numeric(episode))

bakers_df <- bakers_df |>
  rename(baker = baker_name) |> 
  mutate(series = as.numeric(series))
```

Now, we have ensured the consistency of data types. Then, let’s use
‘anti_join’ to check the completeness and correctness across datasets.

We also notice that in bakers_df, bakers have full name recorded.
However, in bakers_df, bakers have first name recorded. We should make
bakers’ names consistent:

``` r
bakers_df <- bakers_df %>%
  mutate(baker = word(baker, 1))
```

Let’s check how many records from the results_df that do not have a
matching “Series” and “Baker” in the bakers_df and vice versa.

``` r
rb_number <- anti_join(results_df, bakers_df, by = c("series", "baker")) |>
  nrow()

rb_number
```

    ## [1] 8

``` r
br_number <- anti_join(bakers_df, results_df, by = c("series", "baker")) |>
  nrow()

br_number
```

    ## [1] 1

Here we can see that 8 records from the results_df that do not have a
matching “Series” and “Baker” in the bakers_df; 1 record from the
bakers_df that do not have a matching “Series” and “Baker” in the
results_df.

After checking, we could merge bakers_df and results_df:

``` r
# Perform the first merge of bakers and results_clean
bakers_results <- results_df |>
  left_join(bakers_df, by = c("series", "baker"))
```

Then, we consider merge bakes_df and bakers_results:

We should also check how many records from the bakers_results that do
not have a matching “Series”, “Episode”, and “Baker” in the bakes_df and
vice versa.

``` r
mb_number <- anti_join(bakers_results, bakes_df, by = c("series", "episode", "baker")) |>
  nrow()

mb_number
```

    ## [1] 596

``` r
bm_number <- anti_join(bakes_df, bakers_results, by = c("series", "episode", "baker")) |>
  nrow()

bm_number
```

    ## [1] 8

Here we can see that 596 records from the bakers_results that do not
have a matching “Series”, “Episode”, and “Baker” in the bakes_df; 8
records from the bakes_df that do not have a matching “Series”,
“Episode” and “Baker” in the bakes_results.

## Merge and organize a single, final dataset

Finally, we can merge bakers_results and bakes_df to get the final
dataset and sort rows by Series, Episode, and Baker to ensure meaningful
order of observations.

``` r
final_data <- bakers_results |>
  left_join(bakes_df, by = c("series", "episode", "baker")) |>
  select(series, episode, baker, baker_age, baker_occupation, hometown, 
         signature_bake, show_stopper, technical, result) |>
  arrange(series, episode, baker)
```

## Export the final data set

``` r
write_csv(final_data, "hw2data/final_bakeoff_data.csv")
```

## Describe the data cleaning process and briefly discuss the final dataset

The data cleaning process began by importing three datasets: bakers.csv,
bakes.csv, and results.csv. Each dataset provided a different aspect of
information about the Great British Bake Off contestants. Once the
datasets were imported, the first task was to clean the results.csv
file, which had irrelevant information in the first two rows and
incorrect column names. To fix this, the first two rows were removed,
and the columns were renamed to series, episode, baker, technical, and
result. Also, we noticed that in bakes.csv file, “Jo” should be Jo,
without quotation marks.

Another issue encountered was the inconsistency in data types for the
series and episode columns across the datasets. In some files, these
were stored as integer values, while in others, they were character
values. To standardize the data, these columns were converted to numeric
type across all datasets, ensuring that the merging process could
proceed smoothly. One of the more significant challenges arose from the
fact that the baker column in bakes.csv and results.csv contained only
first names, while in bakers.csv, full names were used. To resolve this,
the first names were extracted from the baker column in bakers.csv using
the mutate() and word() functions. This allowed the datasets to be
merged using the correct identifier.

After the first step of merging the cleaned results.csv file with
bakers.csv, the second merge was performed with bakes.csv, using both
series, episode, and baker to match contestants’ demographic information
to their performance data.

Once the datasets were merged, the columns were reorganized for better
readability. The baker age, occupation and hometown columns were moved
immediately after the baker column. To further enhance the structure of
the dataset, the data was sorted by series, episode, and baker, ensuring
a logical flow that made it easy to track each contestant’s progress
throughout the competition.

Finally, the cleaned and organized dataset was exported as
final_bakeoff_data.csv. The resulting dataset includes detailed
demographic information about each baker, such as their first name, age,
occupation, and hometown, along with performance data for each episode
in each series, including signature and showstopper bakes, technical
rankings, and overall episode results. The logical structure of the
dataset allows for easy tracking of individual contestants and their
journeys across seasons.

## Create a reader-friendly table

``` r
starbakers_winners <- final_data |>
  filter(series %in% c("5", "6", "7", "8", "9", "10") & 
         result %in% c("STAR BAKER", "WINNER")) |>
  select(series, episode, baker, baker_age, baker_occupation, hometown, result) %>%
  arrange(series, episode)

winners <- starbakers_winners |>
  filter(result == "WINNER")
```

Most of the winners appear to be in their 30s, except for Nancy, who won
Season 5 at the age of 60. To our surprise, the competition is highly
dynamic and the overall winner isn’t always the clear front runner
throughout the season.

## Import, clean, tidy, and organize the viewership data

``` r
viewers_df = read.csv(file = "hw2data/viewers.csv") |>
  janitor::clean_names()
head(viewers_df, 10)
```

    ##    episode series_1 series_2 series_3 series_4 series_5 series_6 series_7
    ## 1        1     2.24     3.10     3.85     6.60    8.510    11.62    13.58
    ## 2        2     3.00     3.53     4.60     6.65    8.790    11.59    13.45
    ## 3        3     3.00     3.82     4.53     7.17    9.280    12.01    13.01
    ## 4        4     2.60     3.60     4.71     6.82   10.250    12.36    13.29
    ## 5        5     3.03     3.83     4.61     6.95    9.950    12.39    13.12
    ## 6        6     2.75     4.25     4.82     7.32   10.130    12.00    13.13
    ## 7        7       NA     4.42     5.10     7.76   10.280    12.35    13.45
    ## 8        8       NA     5.06     5.35     7.41    9.023    11.09    13.26
    ## 9        9       NA       NA     5.70     7.41   10.670    12.65    13.44
    ## 10      10       NA       NA     6.74     9.45   13.510    15.05    15.90
    ##    series_8 series_9 series_10
    ## 1      9.46     9.55      9.62
    ## 2      9.23     9.31      9.38
    ## 3      8.68     8.91      8.94
    ## 4      8.55     8.88      8.96
    ## 5      8.61     8.67      9.26
    ## 6      8.61     8.91      8.70
    ## 7      9.01     9.22      8.98
    ## 8      8.95     9.69      9.19
    ## 9      9.03     9.50      9.34
    ## 10    10.04    10.34     10.05

What was the average viewership in Season 1? In Season 5?

``` r
Series1 = viewers_df |>
  pull(series_1)
S1_mean = mean(Series1, na.rm = TRUE)

Series5 = viewers_df |>
  pull(series_5)
S5_mean = mean(Series5, na.rm = TRUE)
```

The average viewership in Season 1 was 2.77, in Season 5 was 10.0393.
